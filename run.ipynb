{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from typing import NamedTuple\n",
    "import pandas as pd\n",
    "from kfp.components import InputPath, InputTextFile, OutputPath, OutputTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_func(id_data: str, id_label: str, id_pattern: str, path_data: OutputPath(str), path_label: OutputPath(str), path_pattern: OutputPath(str)):\n",
    "    import gdown\n",
    "    \n",
    "    url = f\"https://drive.google.com/uc?id={id_data}\"\n",
    "    print(\"url data:\", url)\n",
    "    gdown.download(url, path_data, quiet=False)\n",
    "    print(\">> Download data success\")\n",
    "    \n",
    "    url = f\"https://drive.google.com/uc?id={id_label}\"\n",
    "    print(\"url label:\", url)\n",
    "    gdown.download(url, path_label, quiet=False)\n",
    "    print(\">> Download label success\")\n",
    "    \n",
    "    url = f\"https://drive.google.com/uc?id={id_pattern}\"\n",
    "    print(\"url pattern:\", url)\n",
    "    gdown.download(url, path_pattern, quiet=False)\n",
    "    print(\">> Download pattern success\")\n",
    "    \n",
    "    import os\n",
    "    import pathlib\n",
    "    print(\"current directory:\", pathlib.Path().resolve())\n",
    "    print(\"os.listdir:\", os.listdir(pathlib.Path().resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_func(path_data: InputPath(str), path_label: InputPath(str), path_pattern: InputPath(str), path_vip_data: OutputPath(str), path_id2label: OutputPath(str)):\n",
    "    \n",
    "    import pandas as pd \n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    \n",
    "    pattern = open(path_pattern).read()\n",
    "    print(\"pattern\", pattern)\n",
    "    \n",
    "    def text_preprocess(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = re.sub(pattern, ' ', text).strip().split()\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "    \n",
    "    import os\n",
    "    import pathlib\n",
    "    print(\"current directory:\", pathlib.Path().resolve())\n",
    "    print(\"os.listdir:\", os.listdir(pathlib.Path().resolve()))\n",
    "    \n",
    "    df_label = pd.read_csv(path_label)\n",
    "    print(\">> Read input_path_label success!!!\")\n",
    "    \n",
    "    dict_label = {}\n",
    "\n",
    "    for index, row in df_label.iterrows(): \n",
    "            name = row['cate_name'].lower()\n",
    "            name = re.sub(pattern, ' ', name).strip().replace('  ', ',')\n",
    "            dict_label[row['cate_id']] = name\n",
    "            \n",
    "    df_data = pd.read_csv(path_data, sep = \"\t\", names=[\"id\", \"cate\", \"name\", \"content\"])\n",
    "    df_data = df_data.dropna(subset=['cate', 'name', 'content'])\n",
    "    name = []\n",
    "    cate = []\n",
    "    content = []\n",
    "    raw_name = []\n",
    "    raw_content = []\n",
    "    for index, row in tqdm(list(df_data.iterrows())):\n",
    "        cates = row['cate'].split(',')\n",
    "        nem = text_preprocess(row['name'])\n",
    "        cont = text_preprocess(row['content'])\n",
    "        for c in cates:\n",
    "            if int(c) in dict_label:\n",
    "                name.append(nem)\n",
    "                cate.append(int(c))\n",
    "                content.append(cont)\n",
    "                raw_name.append(row['name'])\n",
    "                raw_content.append(row['content'])\n",
    "\n",
    "    df_data = pd.DataFrame({\"name\": name, \"cate\": cate, \"content\": content})\n",
    "\n",
    "    rm_index = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        if row['cate'] not in dict_label:\n",
    "            rm_index.append(index)\n",
    "    df_data = df_data.drop(rm_index)\n",
    "    label = [dict_label[i] for i in df_data['cate'].tolist()]\n",
    "    df_data['label'] = label\n",
    "\n",
    "    df_data = df_data[df_data.duplicated('cate',keep=False)]\n",
    "\n",
    "    df_data[\"label\"] = df_data[\"label\"].astype('category')\n",
    "    df_data[\"label_id\"] = df_data[\"label\"].cat.codes\n",
    "    id2label = dict(enumerate(df_data.label.cat.categories))\n",
    "    with open(path_id2label, 'w') as outfile:\n",
    "        json.dump(id2label, outfile)\n",
    "        \n",
    "    print(\">> id2label:\", id2label)\n",
    "\n",
    "    num_classes = len(df_data['label'].unique())\n",
    "    print(\">> num_classes:\", num_classes)\n",
    "                                                                \n",
    "    df_data['text'] = df_data['name'] + ' ' + df_data['content']\n",
    "    \n",
    "    # path_df = 'df_data.csv'\n",
    "    # df_data.to_csv(path_df, index=False)\n",
    "    df_data.to_csv(path_vip_data)\n",
    "    \n",
    "    print(\"current directory:\", pathlib.Path().resolve())\n",
    "    print(\"os.listdir:\", os.listdir(pathlib.Path().resolve()))\n",
    "                                                                        \n",
    "    # from collections import namedtuple\n",
    "    # output = namedtuple(\"output\", [\"path_vip_data\", \"num_classes\"])\n",
    "    # return output(df_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data4predict_func(path_data: InputPath(str), path_label: InputPath(str), path_pattern: InputPath(str), path_vip_data4predict: OutputPath(str)):\n",
    "    import pandas as pd \n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    pattern = open(path_pattern).read()\n",
    "    print(\"pattern\", pattern)\n",
    "    \n",
    "    def text_preprocess(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = re.sub(pattern, ' ', text).strip().split()\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "    \n",
    "    df_label = pd.read_csv(path_label)\n",
    "    print(\">> Read input_path_label success!!!\")\n",
    "    \n",
    "    dict_label = {}\n",
    "    for index, row in df_label.iterrows(): \n",
    "            name = row['cate_name'].lower()\n",
    "            name = re.sub(pattern, ' ', name).strip().replace('  ', ',')\n",
    "            dict_label[row['cate_id']] = name\n",
    "    \n",
    "    \n",
    "    data = pd.read_csv(path_data, sep = \"\t\", names=[\"id\", \"cate\", \"name\", \"content\"])\n",
    "    data = data.dropna(subset=['cate', 'name', 'content'])\n",
    "\n",
    "    new_cate = []\n",
    "    len_cate = []\n",
    "    for index, row in data.iterrows():\n",
    "        n = []\n",
    "        for id in row['cate'].split(','):\n",
    "            if int(id) in dict_label:\n",
    "                n.append(int(id))\n",
    "        new_cate.append(n)\n",
    "        len_cate.append(len(n))\n",
    "\n",
    "    data['cate'] = new_cate\n",
    "    data['len_cate'] = len_cate\n",
    "    data = data[data['len_cate']>0]\n",
    "\n",
    "\n",
    "    data['text'] = data['name'] + ' ' + data['content']\n",
    "    label = [list(set([dict_label[int(i)] for i in cate if int(i) in dict_label])) for cate in data['cate'].tolist()]\n",
    "    data['label'] = label\n",
    "\n",
    "    data = data.reset_index(drop=True)\n",
    "    data.to_csv(path_vip_data4predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_func(path_vip_data: InputPath(str)) -> NamedTuple(\"output\", [(\"path_ckp\", str), (\"path_tokenizer\", str)]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras_preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "    \n",
    "    import os\n",
    "    import pathlib\n",
    "    print(\"current directory:\", pathlib.Path().resolve())\n",
    "    print(\"os.listdir:\", os.listdir(pathlib.Path().resolve()))\n",
    "    print(\"os.listdir /data:\", os.listdir('/data'))\n",
    "    \n",
    "    print(\"path_vip_data:\", path_vip_data)\n",
    "    \n",
    "    df_data = pd.read_csv(path_vip_data)\n",
    "    # df_data = path_vip_data\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = 30\n",
    "    MAX_NB_WORDS = 4200\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS,lower=True)\n",
    "    tokenizer.fit_on_texts(np.array(df_data['text'].tolist()))\n",
    "\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(df_data['text'], df_data['label_id'], \n",
    "                                                                        random_state=2022, \n",
    "                                                                        test_size=0.3, \n",
    "                                                                        stratify=df_data['label_id'])\n",
    "\n",
    "    train_X = train_X.tolist()\n",
    "    val_X = val_X.tolist()\n",
    "    train_Y = train_Y.tolist()\n",
    "    val_Y = val_Y.tolist()\n",
    "\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    train_X = pad_sequences(train_X, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    val_X = pad_sequences(val_X, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    train_X = torch.tensor(train_X)\n",
    "    train_Y = torch.tensor(train_Y)\n",
    "    val_X = torch.tensor(val_X)\n",
    "    val_Y = torch.tensor(val_Y)\n",
    "    \n",
    "    batch_size = 128\n",
    "    workers = 8\n",
    "    train_data = TensorDataset(train_X, train_Y)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=workers)\n",
    "\n",
    "    val_data = TensorDataset(val_X, val_Y)\n",
    "    val_sampler = RandomSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size, num_workers=workers, shuffle=False)\n",
    "    \n",
    "    class Classify(pl.LightningModule):\n",
    "        def __init__(self, n_fea=50, vocab_size=2000, max_length=30, num_classes=300):\n",
    "            super(Classify, self).__init__()\n",
    "            self.embed_layer = nn.Embedding(vocab_size, n_fea)  \n",
    "            self.linear_0 = nn.Linear(max_length*n_fea,1024)\n",
    "            self.linear_1 = nn.Linear(1024,512)\n",
    "            self.linear_2 = nn.Linear(512, num_classes)\n",
    "            self.relu_layer = nn.ReLU()\n",
    "\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        def forward(self, x):\n",
    "            x = self.embed_layer(x)\n",
    "            x = x.view(x.shape[0], 1, -1)\n",
    "\n",
    "            x = self.linear_0(x)\n",
    "            x = self.relu_layer(x)\n",
    "\n",
    "            x = self.linear_1(x)\n",
    "            x = self.relu_layer(x)\n",
    "\n",
    "            x = self.linear_2(x)\n",
    "            return x\n",
    "\n",
    "        def loss(self, logits, labels):\n",
    "            return self.criterion(logits, labels)\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            logits = self.forward(x)\n",
    "            logits = logits.squeeze()\n",
    "            loss = self.loss(logits, y)\n",
    "\n",
    "            self.log(\"train_loss\", loss, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            logits = self.forward(x)\n",
    "            logits = logits.squeeze()\n",
    "            loss = self.loss(logits, y)\n",
    "\n",
    "            self.log(\"val_loss\", loss, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def predict_step(self, batch, batch_idx):\n",
    "            x,y = batch\n",
    "            return self(x)\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    MAX_NB_WORDS = 4200\n",
    "    MAX_SEQUENCE_LENGTH = 30\n",
    "    model = Classify(n_fea=50, vocab_size=MAX_NB_WORDS, max_length=MAX_SEQUENCE_LENGTH, num_classes=92)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=\"/data\",\n",
    "        filename=\"best_ckp\",\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=3, callbacks=[checkpoint_callback])\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    \n",
    "    path_tokenizer = '/data/tokenizer.pickle'\n",
    "    with open(path_tokenizer, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"current directory:\", pathlib.Path().resolve())\n",
    "    print(\"os.listdir:\", os.listdir(pathlib.Path().resolve()))\n",
    "    print(\"os.listdir /data:\", os.listdir('/data'))\n",
    "    \n",
    "    ckp_path = '/data/best_ckp.ckpt'\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple(\"output\", [\"path_ckp\", \"path_tokenizer\"])\n",
    "    return output(ckp_path, path_tokenizer)\n",
    "    \n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_func(path_ckp: str, path_vip_data4predict: InputPath(str), path_id2label: InputPath(str), path_pattern: InputPath(str), path_tokenizer: str):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import pickle\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras_preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "    \n",
    "    import json\n",
    "    import random\n",
    "    \n",
    "    import os\n",
    "    import pathlib\n",
    "    print(\"current directory:\", pathlib.Path().resolve())\n",
    "    print(\"os.listdir:\", os.listdir(pathlib.Path().resolve()))\n",
    "    print(\"os.listdir /data:\", os.listdir('/data'))\n",
    "    \n",
    "    \n",
    "    with open(path_tokenizer, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    pattern = open(path_pattern).read()\n",
    "    def text_preprocess(text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = re.sub(pattern, ' ', text).strip().split()\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "    \n",
    "    class Classify(pl.LightningModule):\n",
    "        def __init__(self, n_fea=50, vocab_size=2000, max_length=30, num_classes=300):\n",
    "            super(Classify, self).__init__()\n",
    "            self.embed_layer = nn.Embedding(vocab_size, n_fea)  \n",
    "            self.linear_0 = nn.Linear(max_length*n_fea,1024)\n",
    "            self.linear_1 = nn.Linear(1024,512)\n",
    "            self.linear_2 = nn.Linear(512, num_classes)\n",
    "            self.relu_layer = nn.ReLU()\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        def forward(self, x):\n",
    "            x = self.embed_layer(x)\n",
    "            x = x.view(x.shape[0], 1, -1)\n",
    "\n",
    "            x = self.linear_0(x)\n",
    "            x = self.relu_layer(x)\n",
    "\n",
    "            x = self.linear_1(x)\n",
    "            x = self.relu_layer(x)\n",
    "\n",
    "            x = self.linear_2(x)\n",
    "            return x\n",
    "\n",
    "        def loss(self, logits, labels):\n",
    "            return self.criterion(logits, labels)\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            logits = self.forward(x)\n",
    "            logits = logits.squeeze()\n",
    "            loss = self.loss(logits, y)\n",
    "\n",
    "            self.log(\"train_loss\", loss, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            logits = self.forward(x)\n",
    "            logits = logits.squeeze()\n",
    "            loss = self.loss(logits, y)\n",
    "\n",
    "            self.log(\"val_loss\", loss, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def predict_step(self, batch, batch_idx):\n",
    "            x,y = batch\n",
    "            return self(x)\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "    def load_model(ckp_path):\n",
    "        MAX_NB_WORDS = 4200\n",
    "        MAX_SEQUENCE_LENGTH = 30\n",
    "        num_classes = 92\n",
    "        device = 'cpu' \n",
    "        model = Classify.load_from_checkpoint(ckp_path, n_fea=50, vocab_size=MAX_NB_WORDS, max_length=MAX_SEQUENCE_LENGTH, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict(model, text, thres=0.8):\n",
    "        MAX_SEQUENCE_LENGTH = 30\n",
    "        text = text_preprocess(text)\n",
    "        toks = tokenizer.texts_to_sequences([text])\n",
    "        toks = pad_sequences(toks, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "        toks = torch.tensor(toks)\n",
    "\n",
    "        m = nn.Sigmoid()\n",
    "        result = {'cate': [], 'score': [], 'ids': []}\n",
    "        with torch.no_grad():\n",
    "            out = model(toks)\n",
    "        out = m(out)\n",
    "        for index, s in enumerate(out[0][0]):\n",
    "            if s>thres:\n",
    "                result['cate'].append(id2label[str(index)])\n",
    "                result['score'].append(round(float(s), 3))\n",
    "        return result\n",
    "    \n",
    "    id2label = json.load(open(path_id2label))\n",
    "    print(\">> id2label\", id2label)\n",
    "    model = load_model(path_ckp)\n",
    "    df_data = pd.read_csv(path_vip_data4predict)\n",
    "    \n",
    "    sample = 50\n",
    "    for _ in range(sample):\n",
    "        index = random.randint(0, 30000)\n",
    "        text = df_data.iloc[index]['text']\n",
    "        label = df_data.iloc[index]['label']\n",
    "        print(text)\n",
    "        print(label)\n",
    "        r = predict(model, text)\n",
    "        print(list(zip(r['cate'], r['score'])))\n",
    "        print('='*50)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"training_pipeline\",\n",
    "              description=\"hello, this is description of training pipeline\")\n",
    "def training_pipeline(id_data=\"1bABs9vHNHJvlZmGzK5kORQtOOkvLel3j\", id_label=\"1bKG4ZsaBAEMRUXKTsHm26Lod5CsfLz-r\", id_pattern=\"19LKS_RBIuAaKjy2gNqrFIUxc0INUQPEe\"):\n",
    "    \n",
    "    data_op = dsl.VolumeOp(name=\"categories-banner-volume\",\n",
    "                           resource_name=\"my-pvc\",\n",
    "                           size=\"4Gi\",\n",
    "                           storage_class=\"openebs-hostpath\",\n",
    "                           modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    download_data_op = kfp.components.func_to_container_op(download_data_func, packages_to_install=[\"gdown\"])\n",
    "    process_data_op = kfp.components.func_to_container_op(process_data_func, packages_to_install=[\"pandas\", \"tqdm\"])\n",
    "    process_data4predict_op = kfp.components.func_to_container_op(process_data4predict_func, packages_to_install=[\"pandas\", \"tqdm\"])\n",
    "    \n",
    "    training_op = kfp.components.func_to_container_op(train_model_func,\n",
    "                                                           packages_to_install=[\"pandas\", \"keras\", \"sklearn\", \"pytorch_lightning\", \"torch\", \"tensorflow-cpu\"])\n",
    "    \n",
    "    predict_op = kfp.components.func_to_container_op(predict_func,\n",
    "                                                           packages_to_install=[\"pandas\", \"keras\", \"sklearn\", \"pytorch_lightning\", \"torch\", \"tensorflow-cpu\"])\n",
    "    \n",
    "    step0 = download_data_op(id_data=id_data, id_label=id_label, id_pattern=id_pattern)\n",
    "    step1 = process_data_op(path_data=step0.outputs['path_data'], path_label=step0.outputs['path_label'], path_pattern=step0.outputs['path_pattern'])\n",
    "    step2 = training_op(step1.outputs['path_vip_data']).add_pvolumes({\"/data\": data_op.volume})\n",
    "    step3 = process_data4predict_op(path_data=step0.outputs['path_data'], path_label=step0.outputs['path_label'], path_pattern=step0.outputs['path_pattern'])\n",
    "    step4 = predict_op(path_ckp=step2.outputs['path_ckp'], path_vip_data4predict=step3.outputs['path_vip_data4predict'], path_id2label=step1.outputs['path_id2label'], path_pattern=step0.outputs['path_pattern'], path_tokenizer=step2.outputs['path_tokenizer']).add_pvolumes({\"/data\": data_op.volume.after(step2)})\n",
    "    \n",
    "    # print(\">>>>\", step2.output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(training_pipeline, \"new_sample_pipeline.yaml\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
